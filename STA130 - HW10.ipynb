{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# The apparent contradiction between a low R^2 (17.6% of variance explained) and large, statistically significant \n",
    "# coefficients can be understood by recognizing that R^2 and p-values assess different aspects of a model. A low R^2 \n",
    "# suggests that the model explains only a small portion of the variability in the outcome (HP), possibly due to the presence\n",
    "# of other unmeasured variables. Meanwhile, large coefficients with significant p-values indicate that the included \n",
    "# variables (e.g., Sp. Def, Generation, and their interaction) do have a statistically meaningful effect on HP, but they \n",
    "# don't account for most of the variation. In essence, the model shows strong evidence for the influence of the predictors, \n",
    "# yet the overall variability in HP is likely driven by additional factors not captured in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 \n",
    "# The design matrix for model4, based on a formula with multiple main effects and interaction terms, leads to a large number\n",
    "# of predictor variables, some of which are complex combinations of scaled and centered variables. This structure introduces\n",
    "# multicollinearity, where high correlations between predictors, especially those involving interactions, result in \n",
    "# instability in the model's coefficient estimates. As a result, small changes in the data can cause large fluctuations in \n",
    "# the model's estimates, making it highly sensitive and prone to overfitting. This instability negatively impacts the \n",
    "# model's ability to generalize to new, unseen data, as it becomes overly reliant on specific patterns in the training data,\n",
    "# reducing its out-of-sample predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e24e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "# Models 5, 6, and 7 represent a series of refinements from models 3 and 4, aimed at improving predictive performance and \n",
    "# stability. Model 5 streamlines the approach by focusing on main predictors and categorical variables, reducing unnecessary\n",
    "# interactions to enhance both in-sample and out-of-sample R^2. Model 6 further refines this by removing less significant\n",
    "# predictors and reducing multicollinearity, resulting in a more stable design matrix. Model 7 reintroduces interaction \n",
    "# terms among key numeric predictors to capture non-linear relationships while retaining significant categorical variables. \n",
    "# By centering and scaling continuous predictors, Model 7 also reduces the condition number, mitigating multicollinearity \n",
    "# and improving model stability. Overall, each model incrementally optimizes predictor selection and interactions, \n",
    "# balancing complexity with predictive accuracy and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcdad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define model formula for model3 as a starting point\n",
    "linear_form = 'HP ~ Attack + Defense + Speed + Legendary + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "\n",
    "# Number of repetitions\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.zeros(reps)\n",
    "out_of_sample_Rsquared = np.zeros(reps)\n",
    "\n",
    "# Run the loop to gather performance metrics\n",
    "for i in range(reps):\n",
    "    # Perform a new 50-50 train-test split for each iteration\n",
    "    pokeaman_training_data, pokeaman_testing_data = train_test_split(pokeaman, test_size=0.5)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    final_model_fit = smf.ols(formula=linear_form, data=pokeaman_training_data).fit()\n",
    "    \n",
    "    # Record the in-sample R-squared\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    \n",
    "    # Calculate and record the out-of-sample R-squared\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(\n",
    "        pokeaman_testing_data.HP, \n",
    "        final_model_fit.predict(pokeaman_testing_data)\n",
    "    )[0, 1] ** 2\n",
    "\n",
    "# Create a DataFrame to store in-sample and out-of-sample R-squared values\n",
    "df = pd.DataFrame({\n",
    "    \"In Sample Performance (R-squared)\": in_sample_Rsquared,\n",
    "    \"Out of Sample Performance (R-squared)\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "# Visualize the results\n",
    "fig = px.scatter(df, x=\"In Sample Performance (R-squared)\", y=\"Out of Sample Performance (R-squared)\",\n",
    "                 title=\"In-Sample vs Out-of-Sample Performance of Model3\",\n",
    "                 labels={\"In Sample Performance (R-squared)\": \"In-Sample R-squared\",\n",
    "                         \"Out of Sample Performance (R-squared)\": \"Out-of-Sample R-squared\"})\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name=\"y=x\", line=dict(color=\"red\", dash=\"dash\")))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bcc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "#The comparison of models 6 and 7, trained on different subsets of Pok√©mon generations, highlights how well these models \n",
    "# generalize when applied to newer generations not included in the training data. Both in-sample and out-of-sample R^2 \n",
    "# values are calculated to assess performance. When trained on Generation 1 data only, both models show good in-sample fit \n",
    "# but poor out-of-sample performance, indicating they struggle to predict HP for later generations. Training on Generations \n",
    "# 1-5 improves generalization, but out-of-sample R^2 for Generation 6 still shows a decline. This underscores the challenge \n",
    "# of extrapolating to future data and emphasizes the importance of training models on a diverse range of generations to \n",
    "# improve their ability to generalize across different data subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9cee32",
   "metadata": {},
   "source": [
    "# https://chatgpt.com/share/6736d392-fc54-8012-b3d5-0584b5e94b12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
